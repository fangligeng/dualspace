{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv \n",
    "import sys \n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_submission(file_to_reorder, newfile_name = \"experiment_results.csv\"):\n",
    "    # READ IN KEYS IN CORRECT ORDER AS LIST\n",
    "    with open('keys.csv','r') as f:\n",
    "        keyreader = csv.reader(f)\n",
    "        keys = [key[0] for key in keyreader]\n",
    "\n",
    "    # READ IN ALL PREDICTIONS, REGARDLESS OF ORDER\n",
    "    with open(file_to_reorder) as f:\n",
    "        oldfile_reader = csv.reader(f)\n",
    "        D = {}\n",
    "        for i,row in enumerate(oldfile_reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            _id, pred = row \n",
    "            D[_id] = pred\n",
    "\n",
    "    # WRITE PREDICTIONS IN NEW ORDER\n",
    "    with open(newfile_name,'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(('Id','Prediction'))\n",
    "        for key in keys:\n",
    "            writer.writerow((key,D[key]))\n",
    "\n",
    "    print(\"\".join([\"Reordered \", file_to_reorder,\" and wrote to \", newfile_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense # For dense layers\n",
    "from keras.models import Sequential # For sequential layering\n",
    "from keras.callbacks import EarlyStopping # For stopping execution\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model_DL(X_train, Y_train, n_nodes, hid_activation = 'sigmoid', out_activation = 'softmax', optimizer = 'adadelta', loss = 'categorical_crossentropy'):\n",
    "    \"\"\" n_nodes is 1-D numpy array with number of nodes on each layer\n",
    "        e.g. [10,20,30] is a model with 3 (hidden) layers,\n",
    "        with 10/20/30 nodes on the first/second/third layers\n",
    "        Returns trained DL model \"\"\"\n",
    "    input_shape = (X_train.shape[1],) # Shape of input data\n",
    "    # Initialize model\n",
    "    model_DL = Sequential()\n",
    "    for i in range(len(n_nodes)):\n",
    "        if i == 0:\n",
    "            # First layer\n",
    "            model_DL.add(Dense(n_nodes[i], activation = hid_activation, input_shape = input_shape))\n",
    "        else:\n",
    "            # Subsequent layers\n",
    "            model_DL.add(Dense(n_nodes[i],activation = hid_activation))\n",
    "    # Output layer\n",
    "    model_DL.add(Dense(15, activation = out_activation))\n",
    "    # Compile model\n",
    "    model_DL.compile(optimizer = optimizer,loss = loss)\n",
    "    # Print model summary\n",
    "    model_DL.summary()\n",
    "    # Early stopping monitor w/ patience=3 (stop after 3 runs without improvements)\n",
    "    early_stopping_monitor = EarlyStopping(patience=5)\n",
    "    # Fit model using 20% of data for validation\n",
    "    model_DL.fit(X_train, Y_train, validation_split=0.2, epochs=200, callbacks=[early_stopping_monitor])\n",
    "    Y_train_DLpred = model_DL.predict(X_train)\n",
    "    acc_DL = accuracy_score(Y_train, Y_train_DLpred)\n",
    "    print('DONE. Accuracy: ', acc_DL)\n",
    "    return model_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntpath\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "def url_domain(url):\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(direc, useFirstThreadOnly):\n",
    "    tags_set = []\n",
    "    values_set = []\n",
    "    classes = []\n",
    "    ids = [] \n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "\n",
    "        in_all_section = False\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))\n",
    "        # accumulate features\n",
    "        tags = \"\"\n",
    "        values = \"\"\n",
    "        for el in tree.iter():\n",
    "            # ignore everything outside the \"all_section\" element\n",
    "            if el.tag == \"all_section\" and not in_all_section:\n",
    "                in_all_section = True\n",
    "            elif el.tag == \"all_section\" and in_all_section:\n",
    "                in_all_section = False\n",
    "                if useFirstThreadOnly is True:\n",
    "                    break\n",
    "            elif in_all_section:\n",
    "                tags += \" \" + el.tag.replace('_', '')\n",
    "                for k, v in el.attrib.iteritems():\n",
    "                    if (\"hash\" and \"id\" and \"index\" and \"size\" and \"time\") not in k:\n",
    "                        if \"file\" in k:\n",
    "                            values += \" \" + path_leaf(v).replace('.', '').replace('$', '').replace('_', '').replace('-', '')\n",
    "                        elif \"url\" in k:\n",
    "                            values += \" \" + url_domain(v).replace('.', '').replace('$', '').replace('_', '').replace('-', '')\n",
    "                        elif (\"key\" or \"name\" or \"target\" or \"command\" or \"socket\" or \"value\") in k:\n",
    "                            values += \" \" + v.replace('.', '').replace('$', '').replace('_', '').replace('-', '')\n",
    "                \n",
    "        tags_set.append(tags)\n",
    "        values_set.append(values)\n",
    "    \n",
    "    assert len(tags_set) == len(values_set)\n",
    "    return tags_set, values_set, np.array(classes), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_bigram(tags_set):\n",
    "    tags_bigram_set = []\n",
    "    for tags_entry in tags_set:\n",
    "        tags = tags_entry.split(' ')\n",
    "        pre_tag = \"\"\n",
    "        tags_bigram = \"\"\n",
    "        for tag in tags:\n",
    "            if pre_tag is not \"\":\n",
    "                tags_bigram += \" \" + pre_tag + tag\n",
    "            pre_tag = tag\n",
    "        tags_bigram_set.append(tags_bigram)\n",
    "    return tags_bigram_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_3gram(tags_set):\n",
    "    tags_3gram_set = []\n",
    "    for tags_entry in tags_set:\n",
    "        tags = tags_entry.split(' ')\n",
    "        first_tag = \"\"\n",
    "        second_tag = \"\"\n",
    "        tags_3gram = \"\"\n",
    "        for tag in tags:\n",
    "            if first_tag is not \"\":\n",
    "                tags_3gram += \" \" + first_tag + second_tag+tag\n",
    "            first_tag = second_tag\n",
    "            second_tag = tag\n",
    "        tags_3gram_set.append(tags_3gram)\n",
    "    return tags_3gram_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(direc, useFirstThreadOnly = False):\n",
    "    tags_set, values_set, classes, ids = get_tokens(direc, useFirstThreadOnly)\n",
    "    tags_bigram = get_tags_bigram(tags_set)\n",
    "    tags_3gram = get_tags_3gram(tags_set)\n",
    "    assert len(tags_set) == len(tags_bigram) == len(tags_3gram)\n",
    "    tokens_set = []\n",
    "    for i in range(len(tags_set)):\n",
    "        tokens_set.append(tags_set[i] + ' ' + values_set[i] + ' ' + tags_bigram[i] + ' ' + tags_3gram[i])\n",
    "    return tokens_set, classes, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#\n",
    "#  Below is main function\n",
    "#\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../train_origin\"\n",
    "test_dir = \"../test_origin\"\n",
    "outputfile = \"experiment_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_classes, _ = get_all_tokens('../train_origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens, _, ids = get_all_tokens('../test_origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3086, 15)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.zeros((len(train_classes),len(util.malware_classes)))\n",
    "y_train[np.arange(len(train_classes)), train_classes] = 1\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = TfidfVectorizer(analyzer = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3086, 68571)\n"
     ]
    }
   ],
   "source": [
    "X_train = TF.fit_transform(train_tokens)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3724, 68571)\n"
     ]
    }
   ],
   "source": [
    "X_test = TF.transform(test_tokens)\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#\n",
    "# Finish collecting data\n",
    "#\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 68571\tAccuracy: 0.85029 (+/- 0.00752)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
    "RF.fit(X_train, y_train)\n",
    "scores = cross_val_score(RF, X_train, y_train, cv=4)\n",
    "print \"Features: \" + str(RF.n_features_) + (\"\\tAccuracy: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "model = SelectFromModel(RF, prefit=True)\n",
    "X_train_new = model.transform(X_train)\n",
    "X_test_new = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 3994\tAccuracy: 0.85483 (+/- 0.00820)\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
    "RF.fit(X_train_new, y_train)\n",
    "scores = cross_val_score(RF, X_train_new, y_train, cv=4)\n",
    "print \"Features: \" + str(RF.n_features_) + (\"\\tAccuracy: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 662\tAccuracy: 0.85418 (+/- 0.01714)\n"
     ]
    }
   ],
   "source": [
    "while X_train_new.shape[1] > 1000:\n",
    "    model = SelectFromModel(RF, prefit=True)\n",
    "    X_train_new = model.transform(X_train_new)\n",
    "    X_test_new = model.transform(X_test_new)\n",
    "    \n",
    "    RF = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
    "    RF.fit(X_train_new, y_train)\n",
    "    scores = cross_val_score(RF, X_train_new, y_train, cv=4)\n",
    "    print \"Features: \" + str(RF.n_features_) + (\"\\tAccuracy: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered tdidf_tokens_rf.csv and wrote to tdidf_tokens_rf_results.csv\n"
     ]
    }
   ],
   "source": [
    "preds_RF = RF.predict(X_test_new)\n",
    "results_RF = np.argmax(preds_RF, axis=1)\n",
    "util.write_predictions(results_RF, ids, \"tdidf_tokens_rf.csv\")\n",
    "reorder_submission(\"tdidf_tokens_rf.csv\", \"tdidf_tokens_rf_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 2000)              7990000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2000)              4002000   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2000)              4002000   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                30015     \n",
      "=================================================================\n",
      "Total params: 16,024,015\n",
      "Trainable params: 16,024,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2468 samples, validate on 618 samples\n",
      "Epoch 1/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 2.1597 - val_loss: 3.3791\n",
      "Epoch 2/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.7916 - val_loss: 2.4329\n",
      "Epoch 3/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.8619 - val_loss: 3.2410\n",
      "Epoch 4/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.8180 - val_loss: 2.6220\n",
      "Epoch 5/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.8337 - val_loss: 2.0483\n",
      "Epoch 6/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.8165 - val_loss: 1.9662\n",
      "Epoch 7/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.8150 - val_loss: 1.6796\n",
      "Epoch 8/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.7334 - val_loss: 2.6932\n",
      "Epoch 9/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.7169 - val_loss: 1.6067\n",
      "Epoch 10/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6949 - val_loss: 1.6649\n",
      "Epoch 11/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6898 - val_loss: 1.6160\n",
      "Epoch 12/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6877 - val_loss: 1.5595\n",
      "Epoch 13/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6810 - val_loss: 1.5508\n",
      "Epoch 14/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6799 - val_loss: 1.5546\n",
      "Epoch 15/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6760 - val_loss: 1.5581\n",
      "Epoch 16/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6739 - val_loss: 1.6370\n",
      "Epoch 17/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6728 - val_loss: 1.6691\n",
      "Epoch 18/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6697 - val_loss: 1.6256\n",
      "Epoch 19/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6684 - val_loss: 1.6836\n",
      "Epoch 20/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6681 - val_loss: 1.5521\n",
      "Epoch 21/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6537 - val_loss: 1.8886\n",
      "Epoch 22/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6590 - val_loss: 1.4948\n",
      "Epoch 23/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6485 - val_loss: 1.4735\n",
      "Epoch 24/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6318 - val_loss: 1.7220\n",
      "Epoch 25/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.6049 - val_loss: 1.4162\n",
      "Epoch 26/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.5865 - val_loss: 1.3678\n",
      "Epoch 27/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.5409 - val_loss: 2.0264\n",
      "Epoch 28/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.5062 - val_loss: 1.3019\n",
      "Epoch 29/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.4291 - val_loss: 1.2825\n",
      "Epoch 30/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.4053 - val_loss: 1.3601\n",
      "Epoch 31/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.3889 - val_loss: 1.5774\n",
      "Epoch 32/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.3296 - val_loss: 1.3331\n",
      "Epoch 33/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.3312 - val_loss: 1.8921\n",
      "Epoch 34/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.3064 - val_loss: 1.1824\n",
      "Epoch 35/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.2594 - val_loss: 1.1233\n",
      "Epoch 36/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.2152 - val_loss: 1.1576\n",
      "Epoch 37/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.2395 - val_loss: 1.4644\n",
      "Epoch 38/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.2206 - val_loss: 1.1514\n",
      "Epoch 39/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1926 - val_loss: 2.0557\n",
      "Epoch 40/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1868 - val_loss: 1.1137\n",
      "Epoch 41/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1744 - val_loss: 1.5709\n",
      "Epoch 42/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1819 - val_loss: 1.4679\n",
      "Epoch 43/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1800 - val_loss: 1.0549\n",
      "Epoch 44/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1301 - val_loss: 1.2038\n",
      "Epoch 45/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1406 - val_loss: 1.1359\n",
      "Epoch 46/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1265 - val_loss: 1.0451\n",
      "Epoch 47/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1194 - val_loss: 1.3103\n",
      "Epoch 48/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1256 - val_loss: 1.1838\n",
      "Epoch 49/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1243 - val_loss: 1.6133\n",
      "Epoch 50/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1184 - val_loss: 1.1179\n",
      "Epoch 51/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1138 - val_loss: 1.2313\n",
      "Epoch 52/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1087 - val_loss: 1.8774\n",
      "Epoch 53/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.1050 - val_loss: 1.3726\n",
      "Epoch 54/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0860 - val_loss: 1.0757\n",
      "Epoch 55/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0724 - val_loss: 1.8725\n",
      "Epoch 56/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0768 - val_loss: 0.9405\n",
      "Epoch 57/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0785 - val_loss: 0.9623\n",
      "Epoch 58/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0532 - val_loss: 0.9431\n",
      "Epoch 59/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0556 - val_loss: 1.1846\n",
      "Epoch 60/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0490 - val_loss: 0.9443\n",
      "Epoch 61/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0404 - val_loss: 1.4471\n",
      "Epoch 62/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0361 - val_loss: 1.2661\n",
      "Epoch 63/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0444 - val_loss: 0.9124\n",
      "Epoch 64/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0408 - val_loss: 1.1090\n",
      "Epoch 65/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0175 - val_loss: 0.8995\n",
      "Epoch 66/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 1.0310 - val_loss: 0.8925\n",
      "Epoch 67/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 1.0101 - val_loss: 0.9014\n",
      "Epoch 68/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 1.0059 - val_loss: 0.9232\n",
      "Epoch 69/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9997 - val_loss: 1.3402\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9993 - val_loss: 1.0206\n",
      "Epoch 71/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 1.0055 - val_loss: 1.1201\n",
      "Epoch 72/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 1.0132 - val_loss: 0.8996\n",
      "Epoch 73/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9949 - val_loss: 0.9376\n",
      "Epoch 74/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9716 - val_loss: 0.8690\n",
      "Epoch 75/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 1.0010 - val_loss: 0.8746\n",
      "Epoch 76/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9824 - val_loss: 0.8675\n",
      "Epoch 77/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9906 - val_loss: 0.9102\n",
      "Epoch 78/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9750 - val_loss: 1.2748\n",
      "Epoch 79/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9902 - val_loss: 1.3863\n",
      "Epoch 80/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9757 - val_loss: 0.8581\n",
      "Epoch 81/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9629 - val_loss: 0.8578\n",
      "Epoch 82/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9475 - val_loss: 0.8983\n",
      "Epoch 83/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9622 - val_loss: 0.9905\n",
      "Epoch 84/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9715 - val_loss: 1.2490\n",
      "Epoch 85/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9669 - val_loss: 1.2545\n",
      "Epoch 86/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9473 - val_loss: 0.8926\n",
      "Epoch 87/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9630 - val_loss: 0.8577\n",
      "Epoch 88/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9460 - val_loss: 0.8603\n",
      "Epoch 89/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9520 - val_loss: 1.4696\n",
      "Epoch 90/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9322 - val_loss: 0.8745\n",
      "Epoch 91/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9389 - val_loss: 1.2018\n",
      "Epoch 92/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9432 - val_loss: 0.8472\n",
      "Epoch 93/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9286 - val_loss: 1.6101\n",
      "Epoch 94/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9566 - val_loss: 0.8410\n",
      "Epoch 95/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9375 - val_loss: 0.8130\n",
      "Epoch 96/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9132 - val_loss: 2.0958\n",
      "Epoch 97/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9350 - val_loss: 1.1680\n",
      "Epoch 98/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9389 - val_loss: 1.0606\n",
      "Epoch 99/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9154 - val_loss: 0.8167\n",
      "Epoch 100/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9138 - val_loss: 1.2997\n",
      "Epoch 101/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9179 - val_loss: 0.8448\n",
      "Epoch 102/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9048 - val_loss: 0.8722\n",
      "Epoch 103/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9168 - val_loss: 0.9386\n",
      "Epoch 104/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9309 - val_loss: 0.9295\n",
      "Epoch 105/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9033 - val_loss: 0.8283\n",
      "Epoch 106/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.8836 - val_loss: 1.1397\n",
      "Epoch 107/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.9084 - val_loss: 0.8052\n",
      "Epoch 108/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9190 - val_loss: 1.1044\n",
      "Epoch 109/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8747 - val_loss: 0.7846\n",
      "Epoch 110/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.9105 - val_loss: 1.3596\n",
      "Epoch 111/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8981 - val_loss: 0.8976\n",
      "Epoch 112/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8831 - val_loss: 1.6752\n",
      "Epoch 113/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8981 - val_loss: 1.0996\n",
      "Epoch 114/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.8660 - val_loss: 0.7635\n",
      "Epoch 115/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8836 - val_loss: 0.8089\n",
      "Epoch 116/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8609 - val_loss: 1.3777\n",
      "Epoch 117/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8840 - val_loss: 0.7849\n",
      "Epoch 118/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8500 - val_loss: 1.5267\n",
      "Epoch 119/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8707 - val_loss: 0.7521\n",
      "Epoch 120/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8583 - val_loss: 0.8395\n",
      "Epoch 121/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8455 - val_loss: 0.7758\n",
      "Epoch 122/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.8722 - val_loss: 0.8241\n",
      "Epoch 123/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8368 - val_loss: 1.3649\n",
      "Epoch 124/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8697 - val_loss: 0.9112\n",
      "Epoch 125/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8458 - val_loss: 1.0478\n",
      "Epoch 126/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8384 - val_loss: 0.8720\n",
      "Epoch 127/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8333 - val_loss: 0.8286\n",
      "Epoch 128/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8312 - val_loss: 1.3728\n",
      "Epoch 129/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8416 - val_loss: 1.2454\n",
      "Epoch 130/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8713 - val_loss: 0.7494\n",
      "Epoch 131/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7985 - val_loss: 0.7668\n",
      "Epoch 132/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8206 - val_loss: 0.8908\n",
      "Epoch 133/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8005 - val_loss: 0.7928\n",
      "Epoch 134/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8314 - val_loss: 0.8806\n",
      "Epoch 135/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8323 - val_loss: 0.7655\n",
      "Epoch 136/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8266 - val_loss: 0.8603\n",
      "Epoch 137/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8042 - val_loss: 0.7500\n",
      "Epoch 138/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7880 - val_loss: 0.7626\n",
      "Epoch 139/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8093 - val_loss: 0.7216\n",
      "Epoch 140/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7952 - val_loss: 0.7396\n",
      "Epoch 141/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8192 - val_loss: 0.7349\n",
      "Epoch 142/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8002 - val_loss: 0.8442\n",
      "Epoch 143/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8125 - val_loss: 0.7332\n",
      "Epoch 144/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7935 - val_loss: 0.7402\n",
      "Epoch 145/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8311 - val_loss: 0.9062\n",
      "Epoch 146/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8056 - val_loss: 0.7256\n",
      "Epoch 147/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7975 - val_loss: 0.7596\n",
      "Epoch 148/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7629 - val_loss: 0.7305\n",
      "Epoch 149/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8197 - val_loss: 0.9112\n",
      "Epoch 150/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7971 - val_loss: 0.7328\n",
      "Epoch 151/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8004 - val_loss: 0.8947\n",
      "Epoch 152/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7950 - val_loss: 1.1250\n",
      "Epoch 153/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7668 - val_loss: 0.7363\n",
      "Epoch 154/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7872 - val_loss: 0.9756\n",
      "Epoch 155/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7654 - val_loss: 0.7201\n",
      "Epoch 156/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7775 - val_loss: 1.5380\n",
      "Epoch 157/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7743 - val_loss: 0.8071\n",
      "Epoch 158/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.8002 - val_loss: 1.5818\n",
      "Epoch 159/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7688 - val_loss: 0.7340\n",
      "Epoch 160/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7676 - val_loss: 0.9476\n",
      "Epoch 161/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7786 - val_loss: 0.7079\n",
      "Epoch 162/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7792 - val_loss: 0.7490\n",
      "Epoch 163/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7824 - val_loss: 1.2099\n",
      "Epoch 164/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7786 - val_loss: 0.7121\n",
      "Epoch 165/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7778 - val_loss: 0.7880\n",
      "Epoch 166/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7619 - val_loss: 0.7215\n",
      "Epoch 167/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7399 - val_loss: 1.2874\n",
      "Epoch 168/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7711 - val_loss: 0.9535\n",
      "Epoch 169/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7842 - val_loss: 0.7179\n",
      "Epoch 170/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7501 - val_loss: 0.7078\n",
      "Epoch 171/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7487 - val_loss: 0.6987\n",
      "Epoch 172/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7655 - val_loss: 1.5238\n",
      "Epoch 173/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7535 - val_loss: 0.7887\n",
      "Epoch 174/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7477 - val_loss: 0.6993\n",
      "Epoch 175/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7335 - val_loss: 2.5447\n",
      "Epoch 176/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7735 - val_loss: 0.7999\n",
      "Epoch 177/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7437 - val_loss: 1.2160\n",
      "Epoch 178/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7465 - val_loss: 1.0130\n",
      "Epoch 179/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7407 - val_loss: 0.6985\n",
      "Epoch 180/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7294 - val_loss: 0.7253\n",
      "Epoch 181/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7551 - val_loss: 0.8067\n",
      "Epoch 182/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7156 - val_loss: 1.3692\n",
      "Epoch 183/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7681 - val_loss: 0.7729\n",
      "Epoch 184/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7462 - val_loss: 0.7979\n",
      "Epoch 185/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7128 - val_loss: 1.0766\n",
      "Epoch 186/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7276 - val_loss: 0.7359\n",
      "Epoch 187/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7401 - val_loss: 0.7017\n",
      "Epoch 188/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7457 - val_loss: 0.8015\n",
      "Epoch 189/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7363 - val_loss: 0.7001\n",
      "Epoch 190/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7153 - val_loss: 0.7030\n",
      "Epoch 191/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7368 - val_loss: 1.1614\n",
      "Epoch 192/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7696 - val_loss: 1.2055\n",
      "Epoch 193/200\n",
      "2468/2468 [==============================] - 12s 5ms/step - loss: 0.7245 - val_loss: 0.6959\n",
      "Epoch 194/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7188 - val_loss: 0.7062\n",
      "Epoch 195/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7318 - val_loss: 0.7069\n",
      "Epoch 196/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7182 - val_loss: 0.6918\n",
      "Epoch 197/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.6998 - val_loss: 0.6972\n",
      "Epoch 198/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7204 - val_loss: 0.6887\n",
      "Epoch 199/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7121 - val_loss: 0.9248\n",
      "Epoch 200/200\n",
      "2468/2468 [==============================] - 11s 5ms/step - loss: 0.7232 - val_loss: 1.2493\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-6b8068193d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_DL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_DL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-cc3ea637d7dd>\u001b[0m in \u001b[0;36mtrain_model_DL\u001b[0;34m(X_train, Y_train, n_nodes, hid_activation, out_activation, optimizer, loss)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel_DL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mY_train_DLpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_DL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0macc_DL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_DLpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DONE. Accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_DL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_DL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fanxu/anaconda2/envs/my-rdkit-env/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fanxu/anaconda2/envs/my-rdkit-env/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "# Use 3994 features\n",
    "model_DL = train_model_DL(X_train_new,y_train,[2000, 2000, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_NN = model_DL.predict(X_test_new)\n",
    "results_NN = np.argmax(preds_NN, axis=1)\n",
    "util.write_predictions(results_NN, ids, \"tdidf_tokens_nn.csv\")\n",
    "reorder_submission(\"tdidf_tokens_nn.csv\", \"tdidf_tokens_nn_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(\"tfidf_train_tokens.npz\", X_train_new)\n",
    "sparse.save_npz(\"tfidf_test_tokens.npz\", X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
